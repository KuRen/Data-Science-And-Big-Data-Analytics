{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nearest Neighbors Algorithm\n",
    "\n",
    "The most common algorithm taught for nearest neighbors is the k-means algorithm.  This uses a distance function and simple alogithm to fit the centers of the k-clusters.  \n",
    "\n",
    "## The Distance Function\n",
    "\n",
    "There are many possible distance functions that one could use, though they may cause your algorithm to never converge. In the popular k-means algorithm the squared euclidean distance is used. \n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "K-means proceeds in a two-step fashion iteratively until it converges.  Convergance is reached when data point assignements don't change from one step to the next.  For k-means convergance is garunteed in finate steps, though no \n",
    "\n",
    "**Assignement** All data points are assigned to their closest cluster defined by the squared euclidean distance. \n",
    "\n",
    "**Update** Cluster centers are updated to be the mean of their data points assigned to them. \n",
    "\n",
    "## Model Criticism\n",
    "\n",
    "We can typically try to evaluate a model internally or externally.  In the external case, we might hold out labeled data.  We could then use the labels to evaluate how well our clustering worked (though if you have labeled data, why not just create a classifier?). In the interanl case, we want to try to use just the data on hand to check the consistancy of the model.  For example, one can fit multiple models on subsets of the data and see how sensitive the cluster centers are to small perterbations in the data.\n",
    "\n",
    "### Global Disimilarity\n",
    "Using our defined distance function, we can calculate the total distance of each data point from their respective cluster center.  The larger this total distance is, the worse the clustering.\n",
    "\n",
    "## Popular Optimizations\n",
    "\n",
    "### Sensible starting points\n",
    "Above, we don't specify how to initialize the clusters.  One common optimization is to try to initialize cluster centers close to where you hypothesize the cluster centers actually are. K-means++ is one such algorithm with a sensible initialization strategy. \n",
    "\n",
    "### Penalizing Large-K\n",
    "When determining the correct K, sometimes we can use theory to guess a reasonable value.  However when trying to determine purely from data alone, it can be helpful to add a penalty term to the objective function to incentivize the minimally necessary K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
